{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Python_summary_Data Cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anotherJoyce/MachineLearning-Project/blob/main/Python_summary_Data_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGRL_nz_lOuT"
      },
      "source": [
        "https://www.cjavapy.com/4/\n",
        "https://www.python-ds.com/python-dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQdZVpOi4e8w"
      },
      "source": [
        "#导入数据包\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5-CI4tz4e81"
      },
      "source": [
        "df1=pd.read_csv(\"resale-flat-prices-2019.csv\")\n",
        "df2=pd.read_csv(\"airbnb_toClean.csv\")\n",
        "header=[\"one\",\"two\",3,4,5,6,7]\n",
        "df3=pd.read_csv(\"acquisitions.csv\",header=None)\n",
        "df3=pd.read_csv(\"acquisitions.csv\",names=header)\n",
        "df4=pd.read_csv(\"salariesMessy.csv\")\n",
        "df5=pd.read_csv(\"y_links.csv\")\n",
        "df6=pd.read_csv(\"absentees2.csv\")\n",
        "# df=pd.read_csv(\"y_links.csv\",parse_dates=[\"publish_time\"])#,index_col=\"publish_time\")\n",
        "#Read csv报错，加上encoding='latin1'\n",
        "# df=pd.read_csv(\"airbnb_clean1.csv\",encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGQoEili4e82"
      },
      "source": [
        "import webbrowser\n",
        "\n",
        "webbrowser.open(\"https://www.youtube.com/watch?v=\"+df.loc[100,\"video_id\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBdFEDKojhfU"
      },
      "source": [
        "# <h3>1.Data Quality Checking<h3/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7vc5abh4e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "outputId": "7d276e88-9885-4730-bc7e-5ad694b40f05"
      },
      "source": [
        "https://www.codeproject.com/Articles/5269227/Cleaning-Data-in-a-Pandas-DataFrame\n",
        "\n",
        "#data type\n",
        "print(df.dtypes)\n",
        "df[\"Salary\"] .astype(float)\n",
        "df.column1.str.isnumeric.sum()\n",
        "#turn string type into numeric\n",
        "govdata=govdata.apply(pd.to_numeric,errors='ignore')\n",
        "pd.to_numeric(df['amount'],errors='raise'))\n",
        "pd.to_numeric(df['amount'],errors='coerce').isnull().value_counts())\n",
        "\n",
        "\n",
        "#decimal\n",
        "df['column_name']=df['column_name'].round(1)  #keep one decimal\n",
        "df.round(1)\n",
        "#?data format content(same unit)\n",
        "\n",
        "#remove spaces\n",
        "df.replace(to_replace=r'^\\s*$',value=np.nan,regex=True,inplace=True)\n",
        "\n",
        "#special symbol,categorical checking(unique categories/replace not unique categories)\n",
        "df.column_name.value_counts()\n",
        "df.column_name=df.column_name.replace({\"Active\":1,\"Completed\":1,\"Signed\":1})\n",
        "df.replace([\"Active\",\"Completed\",\"Signed\"],[\"1\",\"2\",\"3\"])\n",
        "df['amount'].str.replace('$','')\n",
        "#rename columns\n",
        "df= df.rename(columns={0 : 'Missing Values', 1 : '% of Missing Values'})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A    float64\n",
            "B    float64\n",
            "C    float64\n",
            "D    float64\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Salary'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-797ac720ddd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#data type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Salary\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnumeric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#turn string type into numeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Salary'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hss6Rtv4e83"
      },
      "source": [
        "#duplicate columns, duplicate rows\n",
        "#check duplicate column names\n",
        "df.columns.value_counts()\n",
        "df.columns.duplicated()\n",
        "df.duplicated().value_counts()\n",
        "df.drop_duplicates()\n",
        "df[df.duplicated(subset=\"EmployeeNumber\")]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh3c1RXljeAX"
      },
      "source": [
        "# <h3>2.Data Logic Checking<h3/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-HrwbhX4e84"
      },
      "source": [
        "##data logic checking \n",
        "##unique id checking\n",
        "print(\"df\",len(df))\n",
        "print(\"APP_Train.SK_ID_CURR.nunique\",APP_Train.SK_ID_CURR.nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb2T0iKp4e84"
      },
      "source": [
        "<h3>3.Data Cleaning<h3/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42dAyNnz4e84"
      },
      "source": [
        "#change necessary value into nan\n",
        "df.replace(to_replace=r'^\\s*$',value=np.nan,regex=True,inplace=True)\n",
        "df.loc[df['P_USER'] =='U','P_USER'] = np.nan\n",
        "df = df.replace('X',np.nan)\n",
        "df.Salary.replace(0,np.nan,inplace=True)\n",
        "\n",
        "\n",
        "#check na values by columns and specific column\n",
        "print(df.isna().sum())  #如何返回not equal to 0\n",
        "print(df.isna().sum()[\"last_review\"])  \n",
        "#display as percentage \n",
        "missing_per = 100 *df.isnull().sum() /len(df)\n",
        "\n",
        "#check na values by rows and plot histagram\n",
        "#get number of nan by rows  axis=1 means by row\n",
        "APP_Train_NAN_TARGET1=pd.DataFrame(data=APP_Train[APP_Train.TARGET==1].isnull().sum(axis=1),columns=['num'])\n",
        "APP_Train_NAN_TARGET1.hist()\n",
        "\n",
        "#get datatable with nan values in rows\n",
        "nan_rows = df[df.isnull().T.any().T]\n",
        "#drop na\n",
        "df2.dropna(axis=0,how=\"any\",thresh=None,subset=None,inplace=False)\n",
        "dropna(axis=0,how='all',subset=EN_list)\n",
        "\n",
        "##continuous- fillna with mean\n",
        "df[Continuous_List_2] = df[Continuous_List_2].fillna(value=df[Continuous_List_2].mean())\n",
        "df[Continuous_List_2] = df[Continuous_List_2].fillna(df.Continuous_List_2.mean(),inplace=True)\n",
        "df[\"Salary\"].fillna(me, inplace=True)\n",
        "df.loc[(df.Salary ==0)|(df.Salary.isna()),\"Salary\"]=df.Salary[df.Salary !=0].mean()\n",
        "\n",
        "\n",
        "\n",
        "##categorical- fillna with mode\n",
        "\n",
        "#forward and behind method\n",
        "#Use the value in front of the missing value of each column to fill (fill according to the corresponding column, fill before and after the corresponding index)\n",
        "APP_Train_Cleaned.fillna(method='ffill',inplace=True)\n",
        "APP_Train_Cleaned.fillna(method='pad',inplace=True)\n",
        "#Use the value behind the missing value of each column to fill (fill according to the corresponding column, fill before and after the corresponding index)\n",
        "APP_Train_Cleaned.fillna(method='backfill',inplace=True)\n",
        "APP_Train_Cleaned.fillna(method='bfill',inplace=True)\n",
        "\n",
        "df['Test Score'] = df['Test Score'].fillna(df['Test Score'].interpolate()) #avg of above and below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq0_6JCL4e85"
      },
      "source": [
        "<h3>4.Data Processing<h3/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su2dKXzr4e85"
      },
      "source": [
        "#cut categorical variables into range\n",
        "df['C_MONTH']=pd.cut(df['C_MNTH'],[-1,3,6,9,12],labels=['Q1','Q2','Q3','Q4'])\n",
        "\n",
        "##cut categorical values into groups\n",
        "#df.column_name.replace(to_replace=list(set(APP_Train_Cleaned.NAME_TYPE_SUITE.value_counts().index.tolist()) - {'Unaccompanied', 'Family', 'Spouse, partner'}),value='Others',\n",
        "                                      inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91q-mmKHLjDC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g243zwFc4e85"
      },
      "source": [
        "#remove outliers below and above 1.5 IQR\n",
        "IQR=df.likes.quantile(0.75)-df.likes.quantile(0.25)\n",
        "IQR\n",
        "lower_cutoff=df.likes.quantile(0.25)-1.5*IQR\n",
        "upper_cutoff=df.likes.quantile(0.25)+1.5*IQR\n",
        "\n",
        "df[(df.likes>lower_cutoff) & (df.likes<upper_cutoff)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPqDv0lP4e86"
      },
      "source": [
        "##create derivative variables \n",
        "df['neighbour_hood_info']=str(df['neighbour_hood_info'])\n",
        "df['area'],df['neighbour_hood']=df.neighbour_hood_info.str.split(',',1).str\n",
        "df['host_id']=df['host_id'].apply(lambda x:\"h\"+str(x))\n",
        "\n",
        "\n",
        "df['area'] = df.neighbour_hood_info.apply(lambda x: x.split(',')[0])\n",
        "df.groupby(\"column1\").column2.apply(lambda x: x.sum()/x.count())\n",
        "df['Absent'] = df.AbsenceHours.apply(lambda x:\"No\" if x != 0 else \"Yes\") \n",
        "df[\"\"]=df.column1.apply(lambda x: 1 if x>1 else 0)\n",
        "df.Revenue=df.iloc[:,2:].sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvqnroHB4e86"
      },
      "source": [
        "##data scaling\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
        "scaled_X_train = scaler.transform(X_train)\n",
        "scaled_X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "376uDwN74e86"
      },
      "source": [
        "##dummy variables\n",
        "APP_Train_Cleaned= pd.get_dummies(df, columns=Categorical_List, drop_first= True)\n",
        "#dumify the data\n",
        "pd.get_dummies(df.MaritalStatus,prefix=\"MStatus_\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zkeYMb04e86"
      },
      "source": [
        "##keep only needed data\n",
        "\n",
        "df1=pd.DataFrame([\"Unitrend\",\"Burlington\",\"Insight Venture Partners\",\"New\",\"ib\",\"30/10/2013\",\"16000\"])\n",
        "df=df+df1\n",
        "df.pop(\"one\")\n",
        "df.drop(1) # drop certain row\n",
        "\n",
        "df.drop(columns=\"Over18\",axis=1,inplace=True)\n",
        "Df.drop(\"col_name\",axis=1,inplace=True)\n",
        "\n",
        "#remove undesired valued\n",
        "df.drop(df[df.MaritalStatus==\"4\"].index,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTV1gWkb4e87"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGctCp_6jYxH"
      },
      "source": [
        "# <h3>5.Statistical Analysis<h3/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl0VQ37T4e87"
      },
      "source": [
        "#check rows and columns\n",
        "print(df.shape)\n",
        "print(df.describe())   #only for continuous\n",
        "#课上讲了std>mean 说明很分散\n",
        "print(df.info())\n",
        "\n",
        "print(df.comment_count.quantile([0.25,0.75]))\n",
        "print(df.comment_count.quantile(0.75)-df.comment_count.quantile(0.25))\n",
        "\n",
        "#correlation\n",
        "corr_list=APP_Train_Cleaned.corr()['TARGET'].sort_values().index.tolist()[-33:-3]  #按照target 列的corr选择对应列\n",
        "\n",
        "#5. Examine any association between the variables.\n",
        "df.corrwith(df.AbsenceHours).sort_values() # All variables appear to be lowly correlated with absent hours.\n",
        "\n",
        "df[category_var].corr(method=\"spearman\") # assuming the categories are ranked in certain order, still low correlation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eknxWXuG4e87"
      },
      "source": [
        "Numpy Summary \n",
        "np.add(df.col, 10) \n",
        "np.divide(), \n",
        "np.subtract() \n",
        "np.mod(),  \n",
        "np.power(),  \n",
        "np.absolute(),  \n",
        "np.log(),  \n",
        "np.exp(),  \n",
        "np.log(),  \n",
        "np.log10(),  \n",
        "np.sum() \n",
        "np.prod([[11, 22], [3, 4]], axis=1) # row product \n",
        "np.argmax(df.Age) # max age index number \n",
        "df['Age_np'] = np.where(df.Age < 40, \"Forever Young\", \"Not so\") # Data transformation \n",
        "np.extract(df.Age == 19, df.Attrition) # data extraction \n",
        " \n",
        "np.mean(df.col), np.std(), np.var(),  \n",
        " \n",
        "Numpy constant \n",
        "https://numpy.org/doc/stable/reference/constants.html \n",
        "np.nan \n",
        " \n",
        "Numpy Statistics \n",
        "https://numpy.org/devdocs/reference/routines.statistics.html \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRH95nJ1joGO"
      },
      "source": [
        "# <h3>6.Data Exploration<h3/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5VLaN784e88"
      },
      "source": [
        "pd.set_option(\"display.max_columns\",None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14pHEyGm4e88"
      },
      "source": [
        "#define dataframe\n",
        "df = pd.DataFrame({\n",
        "        \"Year\": [2000, 2001, 2002, 2003, 2005],\n",
        "        \"Revenue\": [1400, 1500, 1700, 2000, 2300],\n",
        "        \"Margin\": [0.1, 0.15, 0.08, 0.15, 0.16]})\n",
        "\n",
        "df = DataFrame(np.random.randn(6,4), columns=list('ABCD'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA_uY7Pf4e88"
      },
      "source": [
        "df=df.sort_values('column_name', ascending=False).round(1)  #keep one decimal\n",
        "#display the top 5 most likes videos\n",
        "df.sort_values(by=\"likes\",ascending=False)[:5]\n",
        "\n",
        "list_of_lists = list(map(list,dictionary.items()))\n",
        "print(pd.DataFrame(list_of_lists,columns = [\"Category\",\"Count\"]).sort_values(by = [\"Count\"],ascending = False))\n",
        "df.groupby(\"Reason for Absence\").count().sort_values(by=\"ID\",ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zsqljnm4e89"
      },
      "source": [
        "df_tmp = df_tmp.sort_index()\n",
        "df_tmp = df_tmp.set_index(\"Month\")\n",
        "df.reset_index()   #optional\n",
        "\n",
        "#3. What is the period cover in this dataset?\n",
        "start = df.index.min()\n",
        "end = df.index.max()\n",
        "print(f\"\"\"Start Date: {start.day}-{start.month}-{start.year}\n",
        "End Date: {end.day}-{end.month}-{end.year}\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnHq3IP-4e89"
      },
      "source": [
        "#sliced dataframe\n",
        "#sclied columns with name\n",
        "df[['A','B']]\n",
        "df=df[df.columns[df.columns.isin(['C_TRAF','C_SEV'])]]\n",
        "df=df[df[~df.isin([\"TARGET\"])]]\n",
        "Categorical_List=[\"NAME_CONTRACT_TYPE\",\"WALLSMATERIAL_MODE\",\"EMERGENCYSTATE_MODE\"]\n",
        "df=df.loc[:,Categorical_List]\n",
        "test_corr_list=list(set(corr_list) - {'TARGET'})\n",
        "APP_Test_final=APP_Test_Cleaned.loc[:,test_corr_list]\n",
        "df.loc[:, ['A', 'B']]\n",
        "df.loc[0:2, ['A', 'B']]\n",
        "#sclied columns with location\n",
        "df.iloc[:,1:]\n",
        "df.iloc[:,0]\n",
        "#conditinal columns\n",
        "print(df[df[\"Absenteeism Time in Hours\"] !=0])\n",
        "print(df[['A', 'B']][(df.D > 0) & (df.C < 0)])\n",
        "print(df.loc[(df.room_type==\"Private room\")&(df.price>=80),\"name\"])\n",
        "df = df[df.iloc[:,1] != 0]\n",
        "\n",
        "#sliced rows with location\n",
        "df[1:3]\n",
        "df.loc[index]\n",
        "df.loc[0]\n",
        "df.loc[0,:]\n",
        "df.iloc[3]\n",
        "df=df.iloc[:,[0,1,2,3,4,6]]\n",
        "\n",
        "#fields\n",
        "df.loc[0, 'A']\n",
        "df.iloc[[1,3],[2]]\n",
        "df.iloc[3:5, 0:2]\n",
        "\n",
        "\n",
        "daily=(df[df.Date==nonull_date].loc[:,NAME].values)/(31+31+28)\n",
        "df.loc[df.Date==Jan_date,NAME]=daily*31\n",
        "df.loc[df.Date==Feb_date,NAME]=daily*(28+31)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6G0wk-p4e89"
      },
      "source": [
        "df[df.Age.isna()]\n",
        "df[df.last_review.isna()|df.reviews_per_month.isna()]\n",
        "print(df[df.room_type==\"Private room\"].shape)\n",
        "print(df[df.room_type==\"Private room\"].price.median())\n",
        "print(df[(df.room_type==\"Private room\")&(df.price>=80)])\n",
        "print(APP_Train_NAN_TARGET1[APP_Train_NAN_TARGET1.num<=47].count())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbhtJWPy4e89"
      },
      "source": [
        "##result table processing\n",
        "#change data subtable into data frame\n",
        "APP_Train_NAN_TARGET1=pd.DataFrame(data=APP_Train[APP_Train.TARGET==1].isnull().sum(axis=1),columns=['num'])\n",
        "\n",
        "#可以将value count转成dataframe\n",
        "df3=pd.DataFrame(df0.value_counts().reset_index(name='counts'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93_hD75d4e8-"
      },
      "source": [
        "APP_Train_indexlist_TARGET1=APP_Train_NAN_TARGET1[APP_Train_NAN_TARGET1.num<=47].index\n",
        "APP_Train_indexlist_TARGET0=APP_Train_NAN_TARGET0[APP_Train_NAN_TARGET0.num<=3].index\n",
        "APP_Train_indexlist=APP_Train_indexlist_TARGET1.append(APP_Train_indexlist_TARGET0)\n",
        "#select rows according to index.\n",
        "APP_Train_Cleaned=pd.DataFrame(columns=APP_Train.columns.tolist())\n",
        "for i in range(0,len(APP_Train_indexlist)):\n",
        "  index=APP_Train_indexlist[i]\n",
        "  #APP_Train_Cleaned_TARGET1.loc[i]=(APP_Train.loc[index])\n",
        "  APP_Train_Cleaned=APP_Train_Cleaned.append(APP_Train.loc[index])\n",
        "\n",
        "# Select rows according to index\n",
        "APP_Train_Cleaned_New = APP_Train.iloc[APP_Train_indexlist,:]\n",
        "APP_Train_Cleaned_New.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOcRSfqV4e8-"
      },
      "source": [
        "#check the imbalanced data problems\n",
        "groups = govdata.groupby('C_SEV')\n",
        "groups.size() \n",
        "\n",
        "df_tmp = df_tmp.groupby(['Qtr'], as_index=False).agg(np.mean)  #np.sum\n",
        "#agg by mean, min, max\n",
        "df=df.groupby('SK_ID_CURR',as_index=False).agg({\"MONTHS_BALANCE\":['Mean', 'Min','Max'],\n",
        "                              \"CNT_INSTALMENT\":['Mean', 'Min','Max'],\n",
        "                              \"CNT_INSTALMENT_FUTURE\":['Mean', 'Min','Max'],\n",
        "                              \"NAME_CONTRACT_STATUS\":\"max\",\n",
        "                              \"SK_DPD\":['Mean', 'Min','Max'],\n",
        "                              \"SK_DPD_DEF\":['Mean', 'Min','Max']})\n",
        "df.columns = df.columns.droplevel(0)\n",
        "df.columns = [\"_\".join(x) for x in df.columns.ravel()]\n",
        "\n",
        "#综合\n",
        "df.groupby([\"host_name\"],as_index=False).agg(\"number_of_reviews\").sum().sort_values(\"number_of_reviews\",ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdIy9Msj4e8-"
      },
      "source": [
        "#exploration\n",
        "print(\"Proportion of minority to majority target class\")\n",
        "print(APP_Train.TARGET.value_counts()[1]/APP_Train.TARGET.value_counts()[0])\n",
        "df.groupby(\"Attrition\").Age.describe()\n",
        "df[df.host_id==417504].host_name\n",
        "df.number_of_reviews.max()\n",
        "df.cummin()\n",
        "#DataFrame.cummin(axis=None, skipna=True, *args, **kwargs)\n",
        "df.cummax()  \n",
        "df[df[\"Reason for Absence\"]==23]['Absenteeism Time in Hours'].median()\n",
        "\n",
        "#6. What is the most common reason for absentees?\n",
        "df.Reason.value_counts()[:1] # The most common reason for absence is reason #23, medical consultation\n",
        "\n",
        "# Which month has the lowest employment rate?\n",
        "df[df.emp_combined == df.emp_combined.min()].index.month_name() # April"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLK_tv4e4e8_"
      },
      "source": [
        "#crosstab\n",
        "pd.crosstab(df.Attrition,df.MaritalStatus)\n",
        "pd.crosstab(df.Attrition,df.MaritalStatus).plot.bar()\n",
        "#crosstab mutilple levels\n",
        "pd.crosstab(df.Attrition,[df.MaritalStatus,df.Department])\n",
        "#pivot_table\n",
        "pd.pivot_table(df,index=[\"Attrition\"],columns=[\"Department\"],values=[\"Age\"]) #default is mean\n",
        "pd.pivot_table(df,index=[\"Attrition\"],columns=[\"Department\"],values=[\"Age\"],aggfunc=[\"median\",\"mean\"])\n",
        "df.pivot_table(\"Age\",index=[\"Attrition\"],columns=[\"Department\"]).plot()\n",
        "#8. Which month do we see the highest absentee rates?\n",
        "# Absent hour by employee ID and month\n",
        "pd.pivot_table(df, index=[\"ID\"], columns=[\"month\"], values=[\"AbsenceHours\"]) # Total 36 employees"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzKjcMV04e8_"
      },
      "source": [
        "df = pd.concat([missing_val,missing_per],axis=1)\n",
        "#df = pd.concat([APP_Train_Cleaned_Categorical,APP_Train_Cleaned_Integer_Coded,APP_Train_Cleaned_Continuous_1,APP_Train_Cleaned_Continuous_2],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8CTNMVs4e8_"
      },
      "source": [
        "#时间处理：\n",
        "   df_tmp['Month'] = df_tmp['Date'].apply(lambda x: dt.datetime(x.year, x.month, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czzHVkRm4e8_"
      },
      "source": [
        "\n",
        "#how many videos mentioned about taylar swift in the tags?\n",
        "df[df['tags'].str.contains('Taylor Swift')].shape[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDRvrP6E4e8_"
      },
      "source": [
        "#column's unique values\n",
        "print(df.duplicated(subset=\"neighbour_hood_info\").sum())\n",
        "#获取单列唯一值\n",
        "un=df.drop_duplicates(subset=\"neighbour_hood_info\",keep=\"last\",inplace=True)\n",
        "print(un)\n",
        "df1.duplicated(subset=\"name\").sum()\n",
        "#remove undesired valued\n",
        "df.drop(df[df.MaritalStatus==\"4\"].index,inplace=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddscdzTs4e9A"
      },
      "source": [
        "#data report\n",
        "! pip install pandas_profiling\n",
        "import pandas_profiling as pp\n",
        "report = pp.ProfileReport(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jstmw0jjsTQ"
      },
      "source": [
        "# <h2>7. Datetime</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoFambKp4e9A"
      },
      "source": [
        "import datetime as dt\n",
        "condition1=df.comments_disabled==False\n",
        "condition2=df.ratings_disabled==False\n",
        "len(df[condition1&condition2])\n",
        "\n",
        "start_date = start_date - relativedelta(months=1) \n",
        "\n",
        "# start_date =dt.date.today()-timedelta(days=3)\n",
        "tenor=datetime.datetime.strftime(tenor_month, '%b')   \n",
        "an_array[i]=datetime.strptime(df['Date'][i], '%b. %d')\n",
        "\n",
        "df['day']=df.publish_time.dt.day\n",
        "df['month']=df.publish_time.dt.month\n",
        "df['year']=df.publish_time.dt.year\n",
        "df_tmp['year_week'] = df_tmp['Date'].apply(lambda x:int(str(x.year-1)+str(x.isocalendar()[1]).zfill(2) if x.month==1 and x.isocalendar()[1]==53 else int(str(x.year)+str(x.isocalendar()[1]).zfill(2))))\n",
        "\n",
        "df[\"day_name\"]=df.publish_time.dt.day_name #(locale=\"Chinese\")\n",
        "\n",
        "\n",
        "df_tmp['monthofyear'][i]=date_time.month\n",
        "df_tmp['weekofyear'][i]=date_time.isocalendar()[1]\n",
        "\n",
        "today=dt.date.today()\n",
        "latest_two_week=pd.Timestamp(today-dt.timedelta(days=14))\n",
        "latest_one_week=pd.Timestamp(today-dt.timedelta(days=7))\n",
        "\n",
        "\n",
        "TodaysDate = time.strftime(\"%d_%m_%Y\")\n",
        "tmp.Times = [dt.datetime.strftime(x, '%Y-%m-%d') for x in tmp.Times]\n",
        "df.groupby(df.day_name).count()\n",
        "\n",
        "df.ServiceTime.resample(\"M\").mean().plot() # re-plot by sampling on monthly interval, taking average service time\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "decomposed = seasonal_decompose(df.ServiceTime.resample(\"M\").mean(), model=\"additive\")\n",
        "decomposed.plot() # \n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "decomposed = seasonal_decompose(df.ServiceTime.resample(\"M\").mean(), model=\"multiplicative\")\n",
        "decomposed.plot() # \n",
        "\n",
        "\n",
        "from statsmodels.tsa.stattools import *\n",
        "# 2. Do they exhibit significant trend or seasonality?\n",
        "result = adfuller(df.emp_combined)\n",
        "print(f\"Test Statistic: {result[0]}, p-value: {result[1]}\")# ADF statistic and p-value.\n",
        "\"\"\"\n",
        "Null Hypothesis-> There is a unit root \n",
        "p-value\n",
        "In this case, the test statistic is -3.09969\n",
        "p-value is 0.0265 > 0.05 for 5% significance level, \n",
        "Hence null hypothesis that there is a unit root cannot be rejected \n",
        "so the data is non-stationary implying the presence of trend/seasonality\n",
        "\"\"\"\n",
        "\n",
        "decomposed = seasonal_decompose(df.emp_combined, model=\"additive\")\n",
        "decomposed.plot() # The following plots display the trend and seasonal components found in emp_combined variable.\n",
        "\n",
        "Py resample https://blog.csdn.net/sinat_41701878/article/details/80491631\n",
        "#具体某列的数据聚合\n",
        "df.price.resample('W').sum().fillna(0)   #星期聚合,以0填充NaN值\n",
        "#某两列\n",
        "df[['price','num']].resample('W').sum().fillna(0)\n",
        "#某个时间段内,以W聚合,\n",
        "df[\"2018-5\":\"2018-9\"].resample(\"M\").sum().fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDTLMSCJpl7j"
      },
      "source": [
        "#记录程序运行的时间\n",
        "import time\n",
        "timeEnd=time.perf_counter()\n",
        "timeDur=timeEnd-timeStart\n",
        "print(\"Time taken={:f} seconds\".format(timeDur))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taDYr7irtyBJ"
      },
      "source": [
        "value_list=[0 if np.isnan(i) else i for i in value_list]\n",
        "an_array[i]=stat.stdev(value_list)*math.sqrt(252)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rh3J0jZqlmF"
      },
      "source": [
        "#create dataframe:\n",
        "d=pd.date_range(start_date, end=end_date,freq='M')\n",
        "df = pd.DataFrame({'Date':d})\n",
        "df['Date']=df.Date.apply(lambda x: pd.to_datetime(x))\n",
        "df=df.set_index('Date')\n",
        "df_tmp = pd.DataFrame(Data, index=tmp.Times,columns=tmp.Codes)\n",
        "printdf(df,name=\"\")\n",
        "\n",
        "data2 = {\"X\": list(range(13,104,10)),\n",
        "         \"Y\": list(range(103,1004,100)),\n",
        "         \"Z\": list(range(1003,10004,1000))}\n",
        "index2 = [f\"row_s{i}\" if i <= 5 else f\"row_t{i}\" for i in range(1,11)]\n",
        "df2 = pd.DataFrame(data2, index=index2)\n",
        "Pandas.dataframe.apply()\n",
        "df.axes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df=pd.DataFrame()\n",
        "for i, j in zip(ID_list, NAME):\n",
        "    congestion = get_data_from_wind(i, start_date, end_date, j)\n",
        "    df = pd.concat([df, congestion],\n",
        "                  ignore_index=False, join=\"outer\", axis=1)  #join_axes=[df.index]\n",
        "\n",
        "\n",
        "df_tmp = pd.DataFrame(data, columns=['Date', 'close'])\n",
        "\n",
        "na = pd.DataFrame(np.zeros([len(df_tmp), 1]) * np.nan,columns={np.nan})\n",
        "df_tmp = pd.concat([df_tmp, na], ignore_index=False, join=\"outer\", axis=1)  #,join_axes=[df_tmp.index]\n",
        "\n",
        "df=df.replace('Austria*','Austria')\n",
        "\n",
        "#get max value \n",
        "df_tmp['max']=[None]*len(df_tmp)\n",
        "df_tmp['max'] = df_tmp.max(axis=1)\n",
        "\n",
        "\n",
        "#insert column\n",
        "an_array = [None] * len(df_tmp)\n",
        "for i in range(0, len(df_tmp)):\n",
        "    an_array[i] = df_tmp.iloc[i, :].idxmax(axis=\"columns\")\n",
        "col_no=len(df_tmp.columns)\n",
        "df_tmp.insert(col_no, 'Max_Col', an_array, allow_duplicates=True)\n",
        "\n",
        "an_array =[None]*len(df)\n",
        "this_month=datetime.now().month\n",
        "for i in range(0,len(df)):\n",
        "    an_array[i]=datetime.strptime(df['Date'][i], '%b. %d')\n",
        "    an_array[i] = datetime.date(an_array[i] + timedelta(days=0))\n",
        "    # if an_array[i].month != this_month:\n",
        "    #     an_array[i] = an_array[i].replace(year=2020)\n",
        "    # else:\n",
        "    an_array[i] = an_array[i].replace(year=2021)\n",
        "df.insert(6, 'Date', an_array, allow_duplicates=True)\n",
        "\n",
        "#drop columns\n",
        "if col_name in df.columns:\n",
        "    df.drop(columns=col_name, inplace=True)  \n",
        "\n",
        "df['value']=df.apply(lambda x: x['value'] if pd.isna(x[col_name]) else x[col_name],axis=1) \n",
        "\n",
        "df = content.drop(['source','url'], axis = 1)\n",
        "\n",
        "\n",
        "#rename column:\n",
        "    df.columns = df.columns.get_level_values(0)+'_Demand_Index'\n",
        "\n",
        "#填补一月份\n",
        "index = df.loc[df.month==1].index.tolist()\n",
        "for i in index:\n",
        "    df1.iloc[i,:]=df.iloc[i,:]\n",
        "\n",
        "df=df[(df.quote_date>=start_date)&(df.quote_date<=end_date)]\n",
        "\n",
        "China_Naphtha=China_Naphtha.dropna(axis=1)\n",
        "\n",
        "# clean error data - dont apply to all tables in case too many fillna\n",
        "SHFE_BU_ACTIVE = SHFE_BU_ACTIVE.applymap(lambda x: np.nan if (x=='CWSDService:: Internet Timeout.' or x=='CWSDService:response error.') else x).fillna(method='bfill')\n",
        "\n",
        "CAL_ZCE_PTA_ACTIVE['TA00.CZCoi'] = CAL_ZCE_PTA_ACTIVE['TA00.CZCoi'].fillna(0)\n",
        "calculate_data_af = calculate_data_af.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "\n",
        "history = history.drop_duplicates(subset=['Date'], keep='last')\n",
        "\n",
        "congestion['average'] = congestion.iloc[:, 1:].mean(axis=1)\n",
        "\n",
        "congestion[column_name]= congestion[i].rolling(window=7).mean()\n",
        "# Date = df_tmp.rename(columns={'CLOSE': col_name, 'index': 'Date'})['Date'].tolist()\n",
        "\n",
        "\n",
        "#apply function\n",
        "target_data=['close','volume','oi']\n",
        "for i in target_data:\n",
        "    name_1=str(i)+'_M+1'\n",
        "    name_2=str(i)+'_M+2'\n",
        "    SHFE_BU_ACTIVE[name_1]=SHFE_BU_ACTIVE.apply(lambda x:get_data_final(x.trade_code,i,x.Date, x.Date),axis=1)\n",
        "    SHFE_BU_ACTIVE[name_2]=SHFE_BU_ACTIVE.apply(lambda x:get_data_final(x['Trade_code_M+2'],i,x.Date, x.Date),axis=1)\n",
        "SHFE_BU_ACTIVE=updated(SHFE_BU_ACTIVE_history,SHFE_BU_ACTIVE)\n",
        "\n",
        "calculate_rawdata['B.IPE_volume_sum']=calculate_rawdata.apply(lambda x: 0 if np.isnan(x['B00.IPEvolume']+x['B01.IPEvolume']\n",
        "                                                                            +x['B02.IPEvolume'])\n",
        "                                                              else (x['B00.IPEvolume']+x['B01.IPEvolume']\n",
        "                                                                            +x['B02.IPEvolume']),axis=1)\n",
        "calculate_rawdata['PG.DCE_oi_sum']=calculate_rawdata.apply(lambda x: x['PG00.DCEoi']+x['PG01.DCEoi']\n",
        "                                                                           ,axis=1)\n",
        "\n",
        "df_temp['shift4']=df_temp[column1].shift(4).div(df_temp[column1])\n",
        "df_temp['div']=df_temp.apply(lambda x:x['shift1'] if x['shift1'] is not np.nan else (x['shift2']\n",
        "                                                                                    if x['shift2'] is not np.nan\n",
        "                                                                                    else (x['shift3'] if x['shift3']\n",
        "                                                                                          is not np.nan else (x['shift4'] \n",
        "                                                                                                              if x['shift4']\n",
        "update['2doses%']=update['2doses%'].apply(lambda x: pd.to_numeric(x))\n",
        "\n",
        "\n",
        "df_all['province'] = df_all['warehouse'].map(province_dict) \n",
        "\n",
        "df[df['产量:原油加工量:累计值'].notnull()]\n",
        "\n",
        "\n",
        "value_df = (df.groupby('year_month', as_index=False)\n",
        "       .agg({'SC00.INEvolume':'sum','SC01.INEvolume':'sum',\n",
        "                                                     'SC02.INEvolume':'sum','SC03.INEvolume':'sum','SC.INE_volume_sum':'sum',\n",
        "                                                     'B.IPE_volume_sum','FU00.SHFEvolume','FU01.SHFEvolume',\n",
        "                                                     'FU02.SHFEvolume':'sum','FU_volume_1':'sum','FU_volume_5':'sum',\n",
        "                                                     'FU_volume_9':'sum','LU00.INEvolume':'sum','LU01.INEvolume':'sum',\n",
        "                                                     'LU02.INEvolume':'sum','LU03.INEvolume':'sum','PG00.DCEvolume':'sum',\n",
        "                                                      'PG01.DCEvolume':'sum',\n",
        "                                                     'TA00.CZCvolume':'sum','TA01.CZCvolume':'sum','TA02.CZCvolume':'sum',\n",
        "                                                     'BU00.SHFEvolume':'sum','BU01.SHFEvolume':'sum',\n",
        "                                                       'SC.INE_oi_sum':'mean','FU.SHFE_oi_sum':'mean','LU.INE_oi_sum':'mean',\n",
        "                                                      'PG.DCE_oi_sum':'mean',\n",
        "                                                      'TA.CZC_oi_sum':'mean','BU.SHFE_oi_sum':'mean'})\n",
        "\n",
        "update=update.drop_duplicates(subset=['Country','Date'], keep='first')\n",
        "update=update.sort_values(by=['Country','Date'],ascending=True)\n",
        "\n",
        "\n",
        "#stack\n",
        "   df1 = df1.stack([0,1,2]).reset_index().rename(columns={'level_0':'keys','level_1':'Continent','level_2':'Country',\n",
        "                                                            'level_3':'SubProvince_Department_State'})\n",
        "\n",
        "for i in columns:\n",
        "    df[i] = df[i].map(lambda x:float(x))\n",
        "\n",
        "\n",
        "df['date']=df.date.apply(lambda x: pd.to_datetime(x) if pd.notnull(x) else np.nan)\n",
        "df['date']=df.date.apply(lambda x: dt.date(x.year,x.month,x.day) if pd.notnull(x) else np.nan)\n",
        "df=df[df['value'].notnull()]\n",
        "\n",
        "df_new=pd.DataFrame()\n",
        "for i in year:\n",
        "    df_year=df.loc[df.year==i]\n",
        "    for j in EN_list:\n",
        "        df_year=df_year.sort_values(by='Date')\n",
        "        df_year['day_value']=df_year.apply(lambda x: x[j]/(x['dayofyear']),axis=1)\n",
        "        df_year['day_value']=df_year['day_value'].bfill()\n",
        "        df_year['value']=df_year.apply(lambda x: x['dayofyear']*x['day_value'],axis=1)\n",
        "        df_year['value']=df_year.apply(lambda x: x['value'] if pd.isna(x[j]) else x[j],axis=1)\n",
        "        df_year[j]=df_year['value'].copy()\n",
        "\n",
        "    df_new=df_new.append(df_year)\n",
        "\n",
        "df1=df1[final_list].set_index('Date').sort_index(ascending=True).diff()\n",
        "#求log                                                                                                          is not np.nan\n",
        "df_temp[col_name]= df_temp.apply(lambda x:np.log(x['div']),axis=1)                \n",
        "\n",
        "#merge\n",
        "df=pd.merge(df, elec, left_index=True, right_index=True,how='outer')\n",
        "df_monthly = pd.concat([df_monthly, df_tmp],\n",
        "                      ignore_index=False, join=\"outer\", axis=1)\n",
        "df_updated = pd.concat([history, df],\n",
        "                  ignore_index=False, join=\"outer\", axis=0)\n",
        "df2 =df1.merge(ConvFactor, left_on='Product', right_on='Product')\n",
        "crude_ConvFactor=ConvFactor.loc[ConvFactor.Product=='Crude','Conversion_Factor'].values[0]\n",
        "\n",
        "data_frames=[Steel_mills,fumace,Rebar_production,CDU_run_rate,Bitumen_refineries,Generated_electrical_energy,Pass_veh_whole_sales,Pass_veh_retail_sales,Tyre_factory,\n",
        "MTBE,Pvc_units,Olefin,PX_domestic,Ethylene,Methanol,Pta_usage,\n",
        "Glass_factories,Copolymer_injection,BOPP,Plastic]\n",
        "df_all=reduce(lambdaleft,right:pd.merge(left,right,on=['Date'],\n",
        "how='outer'),data_frames)\n",
        "\n",
        "\n",
        "if df_monthly.iloc[-1].isna().sum()>=1/2*len(df_monthly.iloc[-1]):\n",
        "    df_monthly=df_monthly.iloc[:-1]\n",
        "\n",
        "df10['sum']= df10.sum(axis=1)\n",
        "\n",
        "df_updated=df_updated.drop_duplicates()\n",
        "\n",
        "SHFE_BU_ACTIVE['Trade_code_M+2']=SHFE_BU_ACTIVE['trade_code'].apply(lambda x:'BU'+str(int(x[2:4]))+str(12)+'.SHF' if int(x[5:6])==6 else 'BU'+str(int(x[2:4])+1)+str(0)+str(6)+'.SHF')\n",
        "\n",
        "\n",
        "\n",
        "df 掉转：\n",
        "calculate_rawdata['LU00.INEvolume'][0:70][::1]\n",
        "\n",
        "\n",
        "\n",
        "百分号\n",
        "#df['Per_100_people'].map(lambdax:format(x,'.2%'))\n",
        "\n",
        "df_tmp_month=pd.merge(df_tmp_month,df_tmp_month[data_name].pct_change(),left_index=True,right_index=True,how=\"outer\")\n",
        "df_tmp_month=df_tmp_month.sort_values(['month'])\n",
        "\n",
        "pd.to_numeric(df['Age'], errors='coerce').mean()\n",
        "#df_copy['Age_type'] = df_copy['Age'].apply(lambda x: type(x).__name__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNzMTxyfpuhB"
      },
      "source": [
        "\n",
        "#Series数据结构   index是一列  \n",
        "s=pd.Series([50,72,30,76,80],index=['john','sam','mike','june','lake'],name='x')\n",
        "\n",
        "#通过字典创建\n",
        "dict={'Mon':'123','Tue':345,'Fri':789}\n",
        "M=pd.Series(dict)\n",
        "print(M)\n",
        "N=pd.Series(dict,index=['Mon','Tue','Wed','Thu','Fri','Sat'])\n",
        "print(N)\n",
        "\n",
        "#通过list创建\n",
        "list=[50,72,30,76,80]\n",
        "L=pd.Series(list)\n",
        "print(L)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylGH3siFszXY"
      },
      "source": [
        "df_tmp=df_tmp.round({'B00.IPE': 4, 'B01.IPE': 4,'B02.IPE': 4})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYXfwLqmsDhC"
      },
      "source": [
        "path=r'\\\\glensp_san\\Data\\Data\\Home\\Derivative\\ChinaDOE\\Economic Indicators\\contracts'+'\\\\' +last\n",
        "history=pd.read_excel(path,sheet_name=sheet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCWmfl4TrA6c"
      },
      "source": [
        "#dict\n",
        "data_dict = {'S5708175': 'Steel mills run rate(%)',\n",
        "'S5715650': 'e-furnace run rate(%)',\n",
        "'S5123779': 'Teapots CDU run rate(%)',\n",
        "'S5449386': 'Bitumen refineries run rate (%)',\n",
        "'S0027014':  'Generated Electrical Energy(thousand gwh)',\n",
        "'S6126411':  'Passenger vehicle whole sales(vehicle)',\n",
        "'S6126413': 'Passenger vehicle retail sales(vehicle)',\n",
        "'S6124651': 'Tyre factories run rate (%)',\n",
        "'S5470119': 'MTBE units run rate (%)',\n",
        "'S5475834':  'PVC units run rate (%)',\n",
        "'S5469763':  'Olefin units run rate(%)',\n",
        "'S5448958': 'PX units run rate(%)',\n",
        "'S5474440': 'Ethylene glycol run rate (%)',\n",
        "'S5440915': 'Methanol run rate(%)',\n",
        "'S5446174': 'PTA usage: Zhejiang yarns run rate (%)',\n",
        "'S5914166': 'Glass factories run rate(%)',\n",
        "'S5448967': 'Copolymer injection run rate(%)',\n",
        "'S5448968': 'BOPP run rate(%)',\n",
        "'S5448966': 'Plastic run rate(%)',\n",
        "'S5713307':'Rebar production(ten thousand t)'}\n",
        "\n",
        "Data = list(map(list, zip(*tmp.Data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRReGpVtvrKl"
      },
      "source": [
        "#string\n",
        "for i in MA.columns:\n",
        "    MA = MA.rename(columns={i: i[:-3]})\n",
        "\n",
        "\n",
        "\n",
        "def parse_bilingual(x):\n",
        "    x = x.split('$$')\n",
        "    if x == None:\n",
        "        return ('NA', 'NA')\n",
        "    if len(x) == 1:\n",
        "        return (x[0], 'NA')\n",
        "    return (x[0], x[1])\n",
        "\n",
        "\n",
        "#str中引用\n",
        "def ongoing_parser(product):\n",
        "    sql_query = f\"\"\"\n",
        "    SELECT MAX(extract_date)\n",
        "    FROM chinadoe.ChinaExchangeInventory\n",
        "    WHERE exchange = 'SHFE'\n",
        "    AND contract = '{product}'\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us3QiN3rtTd0"
      },
      "source": [
        "#excel\n",
        "def save_xls(list_dfs, list_name,xls_path):\n",
        "    with ExcelWriter(xls_path) as writer:\n",
        "        for df,name in zip(list_dfs,list_name):\n",
        "            df.to_excel(writer,sheet_name=name,index=False)\n",
        "        writer.save()\n",
        "folder=r'\\\\glensp_san\\Data\\Data\\Home\\Derivative\\ChinaDOE\\Economic Indicators\\contracts\\contracts_'+TodaysDate +'.xlsx'\n",
        "list_dfs=[rawdata,ZEMA,SHFE_BU_ACTIVE,SHFE_FU_ACTIVE,ZCE_PTA_ACTIVE,ZCE_PF_ACTIVE,China_Naphtha]\n",
        "list_name=['rawdata','ZEMA','SHFE_BU_ACTIVE','SHFE_FU_ACTIVE','ZCE_PTA_ACTIVE','ZCE_PF_ACTIVE',\n",
        "           'China_Naphtha']\n",
        "save_xls(list_dfs, list_name,folder)\n",
        "\n",
        "# to_concat_raw = [calculate_rawdata,calculate_zema,CAL_SHFE_BU_ACTIVE,calculate_SHFE_FU_ACTIVE,CAL_ZCE_PTA_ACTIVE,CAL_ZCE_PF_ACTIVE,China_Naphtha]\n",
        "# to_concat = []\n",
        "# for each_table in to_concat_raw:\n",
        "#     table_copy = each_table.copy()\n",
        "#     table_copy = table_copy[~table_copy.index.duplicated(keep='first')]\n",
        "#     to_concat.append(table_copy)\n",
        "# calculate_data= pd.concat(to_concat, ignore_index=False, join=\"outer\", axis=1)\n",
        "# calculate_data=calculate_data.sort_index(ascending=False)\n",
        "\n",
        "\n",
        "writer = pd.ExcelWriter(folder_dir+'\\\\'+'Bloomberg Vaccine Data_'+TodaysDate +'.xlsx', engine='xlsxwriter')\n",
        "\n",
        "update.to_excel(writer, sheet_name='Bloomberg Vaccine Data',index=False)\n",
        "workbook = writer.book\n",
        "worksheet = writer.sheets['Bloomberg Vaccine Data']\n",
        "\n",
        "border_fmt = workbook.add_format({'bottom':1, 'top':1, 'left':1, 'right':1,'align':'center'})\n",
        "worksheet.conditional_format(xlsxwriter.utility.xl_range(0, 0, len(update), len(update.columns)-1), {'type': 'no_errors', 'format': border_fmt})\n",
        "writer.save()\n",
        "\n",
        "\n",
        "#read excel with multiple headers\n",
        "\n",
        "\n",
        "def DemandandIndex():\n",
        "    df=pd.read_excel('rystad realtime - tables to pull_19_03.xlsx',sheet_name= 'country demand',header=[0,1,2,3])\n",
        "    date=df.iloc[:,:2]\n",
        "    date.columns = date.columns.droplevel([0,1,2])\n",
        "    date['Year']=date['Year'].fillna(method='ffill').astype(int)\n",
        "    date['Date']=date.apply(lambda x:str(x.Year)+'-'+x['Month-Day'],axis=1)\n",
        "    date.Date=date.Date.apply(lambda x:datetime.strptime(x,\"%Y-%m-%d\"))\n",
        "    date=date['Date'].reset_index().rename(columns={'index':'keys'})\n",
        "    df1=df.iloc[:,2:]\n",
        "    df1.columns = df1.columns.droplevel(3)\n",
        "    df1 = df1.stack([0,1]).reset_index().rename(columns={'level_0':'keys','level_1':'Continent','level_2':'Country'})\n",
        "    df_final_demand=df1.merge(date, left_on='keys', right_on='keys',how='left')\n",
        "\n",
        "    df_final_demand=df_final_demand.rename(columns={'Jet fuel':'Jet fuel Demand_bbl/d','Diesel':'Diesel Demand_bbl/d',\n",
        "                                     'Gasoline':'Gasoline Demand_bbl/d'})[['Date','Continent','Country','Diesel Demand_bbl/d','Gasoline Demand_bbl/d','Jet fuel Demand_bbl/d']]\n",
        "    print(df_final_demand)\n",
        "\n",
        "\n",
        "#csv\n",
        "df.to_csv(folder + '\\\\' + 'covid19_us_cdc_data.csv',mode='a',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiZHH0z3tWBk"
      },
      "source": [
        "pd.set_option('display.max_columns',None)\n",
        "pd.set_option('display.max_rows',None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_O2RIIixsn9"
      },
      "source": [
        "#parse\n",
        "url = 'https://covid.cdc.gov/covid-data-tracker/COVIDData/getAjaxData?id=us_trend_data'\n",
        "content = requests.get(url).content\n",
        "\n",
        "parsed = json.loads(content)\n",
        "data_list = parsed['us_trend_data']\n",
        "if len(data_list) > 1:\n",
        "    df = pd.DataFrame(data_list)\n",
        "\n",
        "\n",
        "url=\"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv\"\n",
        "df=pd.read_csv(url)\n",
        "\n",
        "\n",
        "cob_string = datetime.strftime(cob, '%Y%m%d')\n",
        "response = requests.get(f'http://www.shfe.com.cn/data/dailydata/{cob_string}weeklystock.dat')\n",
        "# Non business day, html returned\n",
        "print(response.text)\n",
        "if '!DOCTYPE HTML PUBLIC' in response.text:\n",
        "    return None\n",
        "json_data = json.loads(response.text)\n",
        "data_list = json_data['o_cursor']\n",
        "# returns json but no valid data\n",
        "if len(data_list) > 1:\n",
        "    df = pd.DataFrame(data_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTOvr2u2bZyS"
      },
      "source": [
        "reorganize onenote"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "DC2Qg_0gbcuh",
        "outputId": "0f33377d-2086-40f4-f8ae-d9058a5e9d42"
      },
      "source": [
        "\n",
        "df = pd.DataFrame(np.random.randn(6,4), columns=list('ABCD'))\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.042516</td>\n",
              "      <td>0.590694</td>\n",
              "      <td>-1.211327</td>\n",
              "      <td>1.979588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.281765</td>\n",
              "      <td>1.304558</td>\n",
              "      <td>0.280585</td>\n",
              "      <td>-0.688735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.869873</td>\n",
              "      <td>0.079664</td>\n",
              "      <td>1.678064</td>\n",
              "      <td>0.674405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.756135</td>\n",
              "      <td>-0.480714</td>\n",
              "      <td>1.097407</td>\n",
              "      <td>-0.471614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.646476</td>\n",
              "      <td>0.753053</td>\n",
              "      <td>0.245742</td>\n",
              "      <td>-0.566194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.582585</td>\n",
              "      <td>-0.817012</td>\n",
              "      <td>-1.534313</td>\n",
              "      <td>-1.536208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          A         B         C         D\n",
              "0  0.042516  0.590694 -1.211327  1.979588\n",
              "1  2.281765  1.304558  0.280585 -0.688735\n",
              "2 -0.869873  0.079664  1.678064  0.674405\n",
              "3 -1.756135 -0.480714  1.097407 -0.471614\n",
              "4 -0.646476  0.753053  0.245742 -0.566194\n",
              "5  1.582585 -0.817012 -1.534313 -1.536208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGP0u8UNc3-w"
      },
      "source": [
        "String"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twjI8Qu5c03C"
      },
      "source": [
        "\n",
        "letters=\"441202199611171026z\"\n",
        "\n",
        "#多行string的展示（保持行距）\n",
        "idcard=\"\"\"\n",
        "        '441202199611171026','440224199609192853',\n",
        "        '440507199612190314','440583199808112533',\n",
        "        '440982199709015629','440982199710292797\"\"\"\n",
        "print(idcard)\n",
        "#输出长度\n",
        "print(len(letters))\n",
        "\n",
        "#1.1学习字符串单个/连续查找\n",
        "#单值索引有分顺序和倒序\n",
        "print(letters[0])\n",
        "print(letters[-1])\n",
        "#查找是否有特定单个值，返回位置\n",
        "print(letters.index('4'))\n",
        "#查找字符串中的某段是否有某个值，用str.index(str, beg=0, end=len(string))表示开始，结束\n",
        "print(letters.index('7',5))\n",
        "\n",
        "#连续值索引，也有顺序和倒序：注意这里是输出到最后一个值-1的地方\n",
        "print(letters[1:3])\n",
        "print(letters[-3:-1])\n",
        "#注意不遵守规则是空值\n",
        "print(letters[-1:-8])\n",
        "print(letters[3:1])\n",
        "#查找是否有特定连续值？返回位置\n",
        "print(letters.index('1996'))\n",
        "#按规律跳step索引\n",
        "print(letters[0:6:2])\n",
        "\n",
        "#如果数值串中下一个值和上一个值相同，输出这个值的位置\n",
        "\n",
        "#1.2字符串中的判断\n",
        "#具体位置上是否为某值\n",
        "print(letters[0]=='0')\n",
        "#判断是否都为大小写\n",
        "print(letters.islower())\n",
        "print(letters.isupper())\n",
        "#判断是否为字母或者数字\n",
        "print(f\"是否为字母{letters.isalpha()}\")\n",
        "#print(f\"是否为数字{letters.digit()}\")\n",
        "\n",
        "\n",
        "#1.3字符串变换\n",
        "#变成规范文书格式，首字母大写，其他小写\n",
        "print(letters.capitalize())\n",
        "#字符串中的字母变成大小写\n",
        "print(letters.upper())\n",
        "print(letters.lower())\n",
        "#改变字符串中具体某个值\n",
        "#letters.replace(4,Z) 这种写法报错，因为是字符串不是数字\n",
        "#只会在具体函数里改，并没有改变letters的原值\n",
        "print(letters.replace(\"4\",\"z\"))\n",
        "\n",
        "\n",
        "#1.4字符串运算\n",
        "print(letters*5)\n",
        "print(letters+letters)\n",
        "#print(letters-letters) 报错，不能减\n",
        "\n",
        "\n",
        "#1.5统计出现的每个值的个数\n",
        "\n",
        "\n",
        "#学习字符串格式\n",
        "\n",
        "companyName= \"MSBA Enterprise\"\n",
        "##append companyName to the string \n",
        "reportHeader = f\"Revenue Report :{companyName}\" \n",
        "##在python中string也可以用这种快速方法print\n",
        "pgno = 1\n",
        "toPrintPage = f\"Page {pgno}\" \n",
        "print(\"=\"*100) \n",
        "print(f\"{reportHeader:<30}\") \n",
        "print(f\"{reportHeader:>100}\") \n",
        "#center-justified to 80 characters\n",
        "print(f\"{reportHeader:^80}\") \n",
        "print(\"=\"*100) \n",
        "#下面这个应该输出'' ，但是没输出呀？\n",
        "print(toPrintPage[-2:-1])\n",
        "#下面这个应该输出ag\n",
        "print(toPrintPage[1:3])\n",
        "#下面这个应该输出1，用于只是取最后页数，因为page固定，所以不论页数多大，都能取到\n",
        "print(toPrintPage[5:])\n",
        "#下面这个应该输出1\n",
        "print(toPrintPage[-1:])\n",
        "# toPrintPage[-1: ] will do also, but cannot handel page10，页数大的话无法推测\n",
        " \n",
        "\n",
        "toPrintUni = \"NTU\" \n",
        "#string的拼接\n",
        "print(f\"{toPrintUni:<20}{reportHeader:^60}{toPrintPage:>20}\")\n",
        "# Left align (<), right align (>) and center align (^).\n",
        "print(f\"{toPrintUni:<20}{reportHeader:^60}{toPrintPage[0]+toPrintPage[1]+toPrintPage[2]+toPrintPage[3]:>20}\")\n",
        "## f{}和{}.format()一样吗 一样\n",
        "print(\"{0:<20}{1:^60}{2:>20}\".format(toPrintUni,reportHeader,toPrintPage))\n",
        "\n",
        "#print(\"{0:<20}{1:^60}{2:>20}\".format(toPrintPage[1]+toPrintPage[1]))\n",
        "\n",
        "\n",
        "\n",
        "for c in sentence:\n",
        "    if c.isupper(): numUpper+=1\n",
        "    if c.islower(): numLower+=1\n",
        "print(f\"\"\"\n",
        "The total upper case is:{numUpper} \n",
        "The total lower case is:{numLower} \n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMT8l4mrjysn"
      },
      "source": [
        "# List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWPjA_60dRRb"
      },
      "source": [
        "# primeList=list(range(n+1))   #gives 0,1,2,...n\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Aug  5 12:25:29 2020\n",
        "\n",
        "@author: joyce\n",
        "\"\"\"\n",
        "\n",
        "del list 清除定义变量\n",
        "\n",
        "List的最大特点：可变更值\n",
        "my_list = [1, 2, 3]\n",
        "my_list[0] = 127\n",
        "print(my_list)  结果为 [127, 2, 3]\n",
        "\n",
        "#来学学list的用法啦\n",
        "idcard=['441202199611171026','440224199609192853',\n",
        "        '440507199612190314','440583199808112533',\n",
        "        '440982199709015629','440982199710292797']\n",
        "一个list赋值给另一个:\n",
        "y2=y1[:] \n",
        "list赋range值:\n",
        "s=list(range(1,10))\n",
        "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "只有满足条件（非必须）的会被加到list里面，这种叫做list comprehension（很特别的python特有东东）\n",
        "List_name=[data for data in iterator if condition]\n",
        "two_multiples=[i*2 for i in range(10)]\n",
        "print(two_multiples)\n",
        "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
        "divible_by_three=[j for j in range(10) if j%3==0]\n",
        "[0, 3, 6, 9]\n",
        "print(divible_by_three)\n",
        "\n",
        "\n",
        "\n",
        "list1=[10,20,30]\n",
        "list的复合 也就是list中还有list \n",
        "a=[[1,2], [2,3]]\n",
        "My_list = ['a', [1, 2, 3], 'z']\n",
        "\n",
        "输出list的所有元素：\n",
        "a=[[1,2], [2,3],1.0]\n",
        "for element in a:\n",
        " print(element,end=' ')\n",
        "\n",
        "判断list中是否存在该值   1 in [1, 2, 3] => True\n",
        "\n",
        "\n",
        "#查看长度\n",
        "print(len(list1))\n",
        "a=[[1,2], [2,3]]\n",
        "print(len(a))   2\n",
        "\n",
        "\n",
        "#1.1学习list单个/连续查找\n",
        "#单值索引有分顺序和倒序\n",
        "print(idcard[0])\n",
        "[1,'a',3] [1]=='a' \n",
        "print(idcard[-1])\n",
        "#查找是否有特定单个值，返回位置\n",
        "print(idcard.index('440507199612190314'))\n",
        "#查找某段是否有某个值，用beg=0, end=len()表示开始，结束\n",
        "print(idcard.index('440507199612190314',2))\n",
        "\n",
        "#连续值索引，也有顺序和倒序：正序从0开始，倒序从-1开始,注意这里是输出到最后一个值-1的地方，如 list[a:b]表示从list[a]到list[b-1]的数\n",
        "print(idcard[1:3])\n",
        "print(idcard[-3:-1])\n",
        "#注意不遵守规则是空值\n",
        "print(idcard[-1:-8])\n",
        "print(idcard[3:1])\n",
        "list[:2 ]\n",
        "#按规律跳step索引\n",
        "print(idcard[0:6:2])\n",
        "#list的复合 这时候list的位置用两位表示，定位到复合list中的情况\n",
        "my_list[1][0]=1\n",
        "a=['a',[1,2,3],'z']\n",
        "print(a[1][1])  结果为2\n",
        "a=[[1,2], [2,3]]\n",
        "print(a[1][1])  结果为3\n",
        "\n",
        "\n",
        "#如果数值串中下一个值和上一个值相同，输出这个值的位置\n",
        "\n",
        "#1.2list中的判断\n",
        "#具体位置上是否为某值\n",
        "print(idcard[0]=='441202199611171026')\n",
        "#查找list中的最大最小值\n",
        "min最小值 max最大值（都要求数据类型相同） sum 只有数值型可以\n",
        "a=[[1,2], [2,3]]\n",
        "print(min(a))   [1,2]\n",
        "print(max(a))    [2,3]\n",
        "\n",
        "\n",
        "\n",
        "#1.4list运算\n",
        "print(idcard*2)\n",
        "print(idcard+idcard)\n",
        "#print(idcard-idcard) 报错，不能减\n",
        "#list加减\n",
        "两个list相+   [1, 2, 3] + [4] => [1, 2, 3, 4]\n",
        "list相*[1, 2, 3] * 2 => [1, 2, 3, 1, 2, 3]\n",
        "元素加减: print(\"元素的加减\",list1[0]+list1[1])\n",
        "#最大最小值\n",
        "print(min(list1))\n",
        "print(max(list1))\n",
        "#总和\n",
        "print(sum(list1))\n",
        "#在最后添加值\n",
        "list1.append(50)\n",
        "print(f\"append后的{list1}\")\n",
        "my_list.extend()\n",
        "#sort后的\n",
        "list1.sort()\n",
        "print(f\"sort后的{list1}\")\n",
        "#list.insert(index, obj),在哪个位置后面插入某值\n",
        "list1.insert(2,60)\n",
        "my_list.remove()\n",
        "print(f\"insert后的{list1}\")\n",
        "#reverse后的 调转顺序\n",
        "list1.reverse()\n",
        "print(f\"reverse后的{list1}\")\n",
        "my_list.append(), \n",
        "my_list.pop() #remove last object\n",
        "#把lis中的元素换位置\n",
        "z=idcard[0]\n",
        "new=idcard[1:7].append(z)\n",
        "print(new)\n",
        "\n",
        "#1.5统计出现的每个值的个数\n",
        "#在字符串中找数字\n",
        "If List1.isdigit()\n",
        "find_digit=[s for s in \"1,'a',2,3,4\" if s.isdigit()]\n",
        "print(find_digit)\n",
        "['1', '2', '3', '4']\n",
        "\n",
        "\n",
        "##综合练习1\n",
        "stock=[\"Co B\",\"Co A\"]\n",
        "lots=[50,100]\n",
        "bi=stock.index(\"Co B\")\n",
        "lots[bi]+=30\n",
        "#above is \n",
        "\n",
        "ai=stock.index(\"Co A\")\n",
        "lots[ai]+=5\n",
        "\n",
        "print(stock)\n",
        "print(lots)\n",
        "totalLost=sum(lots)\n",
        "print(\"total lots=\",totalLost)\n",
        "\n",
        "minLots=min(lots)\n",
        "mi=lots.index(minLots)\n",
        "company=stock[mi]\n",
        "print(\"company with min lots=\",company)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "List元素相加s\n",
        "46//10 6 \n",
        "Return 10-6=4\n",
        "\n",
        "\n",
        "# range也是一种数据类型，有长度为4。\n",
        "\n",
        "# Range 和list的比较。range是数据范围，list是数组\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyt3Uu6Gj2kn"
      },
      "source": [
        "#Tuple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdosgyIlda7j"
      },
      "source": [
        "list和turple的区别: turple初始化后不能改。\n",
        "然而如果是turple中有list,list的内容可以改。 list中有turple，turple的内容不能改。\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jul 28 14:59:56 2020\n",
        "\n",
        "@author: joyce\n",
        "\"\"\"\n",
        "#来学学tuple的用法啦  最大区别是tuple可以放不同数据类型的\n",
        "idcard_tuple=(123,\n",
        "        '440507199612190314','440583199808112533',\n",
        "        '440982199709015629','440982199710292797')\n",
        "list1=(10,20,30)\n",
        "\n",
        "#转化成tuple类型 tuple()\n",
        "#输出长度\n",
        "print(len(list1))\n",
        "\n",
        "#if function return multiple values, it is in tuple form\n",
        "def calc_maths(x,y):\n",
        "    add_result=x+y\n",
        "    substract_result=x-y\n",
        "    return add_result,substract_result\n",
        "\n",
        "print(calc_maths(10,5))\n",
        "\n",
        "#查看长度\n",
        "print(len(idcard_tuple))\n",
        "\n",
        "#1.1学习tuple单个/连续查找\n",
        "#单值索引有分顺序和倒序\n",
        "print(idcard_tuple[0])\n",
        "print(idcard_tuple[-1])\n",
        "#查找是否有特定单个值，返回位置\n",
        "print(\"查找，返回元素位置\")\n",
        "print(idcard_tuple.index('440507199612190314'))\n",
        "#查找某段是否有某个值，用beg=0, end=len()表示开始，结束\n",
        "print(idcard_tuple.index('440507199612190314',0))\n",
        "\n",
        "#连续值索引，也有顺序和倒序：注意这里是输出到最后一个值-1的地方\n",
        "print(idcard_tuple[1:3])\n",
        "print(idcard_tuple[-3:-1])\n",
        "#注意不遵守规则是空值\n",
        "print(idcard_tuple[-1:-8])\n",
        "print(idcard_tuple[3:1])\n",
        "#按规律跳step索引\n",
        "print(idcard_tuple[0:6:2])\n",
        "\n",
        "\n",
        "#1.2tuple中的判断\n",
        "#具体位置上是否为某值\n",
        "print(idcard_tuple[0]=='441202199611171026')\n",
        "\n",
        "#1.4tuple运算\n",
        "print(f\"tuple运算{idcard_tuple*2}\")\n",
        "print(idcard_tuple+idcard_tuple)\n",
        "#print(idcard_tuple-idcard_tuple) 报错，不能减\n",
        "#tuple元素的加减\n",
        "print(\"元素的加减\",list1[0]+list1[1])\n",
        "#最大最小值\n",
        "print(min(list1))\n",
        "print(max(list1))\n",
        "#总和\n",
        "print(sum(list1))\n",
        "#sorted pdf上说有，有吗\n",
        "print(f\"sorted后的{list1}\")\n",
        "#计算tuple中包含某值的个数\n",
        "print(\"包含某值的个数\")\n",
        "print(idcard_tuple.count(123))\n",
        "print(list1.count(10))\n",
        "#把lis中的元素换位置\n",
        "z=idcard_tuple[0]\n",
        "print(new)\n",
        "\n",
        "#1.5统计出现的每个值的个数\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jtAyZajj7HH"
      },
      "source": [
        "# Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR7FTRlJdgXA"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Aug  5 15:29:00 2020\n",
        "\n",
        "@author: joyce\n",
        "\"\"\"\n",
        "#来学学dictionary的用法啦  \n",
        "#可以定义空的\n",
        "x1={}\n",
        "#可以一个个定义\n",
        "x1={}\n",
        "x1[\"1\"]=\"James\"\n",
        "x1[\"2\"]=\"Amy\"\n",
        "x1{'1': 'James', '2': 'Amy'}\n",
        "\n",
        "#可以一连串定义\n",
        "x5={\"2\":\"banana\",\"3\":\"cherry\",4:\"durian\",5.3346:\"pineapple\",\"liulian\":\"durion\",\n",
        "    \"jiagong\":\"fruit\"}\n",
        "print(x5)\n",
        "m=x5[\"liulian\"]\n",
        "print(m)\n",
        "x6={\"apple\":(\"eh-perl\",\"a kind of red fruit\",\"green apple\",\"orange\"),\n",
        "    \"banana\":(\"ber-eh-perl\",\"long yellowish delicious fruits\",\"mango\",\"watermelon\")\n",
        "   }\n",
        "#可以按条件定义\n",
        "Dict_name={key: value for data in iterator if condition }\n",
        "dict_data={\"T\":'Apple',\"D\":'Pear',\"E\":'Banana'}\n",
        "data_processed={key:value.lower() for key,value in dict_data.items()}\n",
        "print(data_processed)\n",
        "{'T': 'apple', 'D': 'pear', 'e': 'banana'} \n",
        "data_processed={key:value.lower() for key,value in dict_data.items() if key in \"TD\"}\n",
        "print(data_processed)\n",
        "{'T': 'apple', 'D': 'pear'}\n",
        "data_processed={key:key for key in dict_data if key.islower()}\n",
        "#查看所有items\n",
        "print(\"查看items\")\n",
        "print(x6.items())\n",
        "\n",
        "#查看所有key\n",
        "print(\"查看所有key\")\n",
        "print(x6.keys())\n",
        "\n",
        "#查看所有value\n",
        "print(\"查看所有value\")\n",
        "print(x6.values())\n",
        "\n",
        "\n",
        "#value的索引\n",
        "key=\"banana\"\n",
        "print(\"meaning of\",key,\"=\",x6[key][1])\n",
        "\n",
        "print(\"similar kind of fruit as \",key,sep=\" \",end=\"\")\n",
        "print(\" is \",x6[key][2],sep=\"\")\n",
        "print(f\"a very different fruit from {key} is {x6[key][3]}...\")\n",
        "\n",
        "\n",
        "#查看长度\n",
        "print(len(x6))\n",
        "\n",
        "#清空所有值\n",
        "print(x6.clear())\n",
        "\n",
        "\n",
        "基本建立字典语法:\n",
        "sgToCur={'USD':0.73829,\n",
        "            'MYR':3.05657,\n",
        "            'EUR':0.66323,\n",
        "            'GBP':0.55172,\n",
        "            'AUD':1.07305}\n",
        "sgToCur['MYR']=0.18   可以直接加1个\n",
        "\n",
        "Print 字典：#可以定义key和value\n",
        "for cur,rate in sgToCur.items():\n",
        "    print(f\"{'':20s}{cur}:{rate:2f}\") \n",
        "    print(f\"{'':20s}{cur}:{rate:2f}\")\n",
        "    print(\"{:20s}{}:{:2f}\".format('',cur,rate))\n",
        "\n",
        "\n",
        "\n",
        "sgToCur={'USD':0.73829,\n",
        "         'MYR':3.05657,\n",
        "         'EUR':0.66323,\n",
        "         'GBP':0.55172,\n",
        "         'AUD':1.07305}\n",
        "sgToCur['test']=0\n",
        "menu=\"\"\"Welcome to theForex Calculator\n",
        "The rates are shown as below:\n",
        "\"\"\"\n",
        "print(menu)\n",
        "\n",
        "for cur,rate in sgToCur.items():\n",
        "    print(f\"{'':20s}{cur}:{rate:7.5f}\") #这里为什么用了单引号，因为前面是双引号，最好区别开。也可，但是最好和前面不一样。\n",
        "    #7 characters space\n",
        "    #这里20s可以是20f吗？可以，但竟然不会报错，但是在format里面会\n",
        "    #format会检查出来string不是float\n",
        "    #但注意s不能去掉。我的疑问：d, s f分别是什么意思？应该是integer,string, float的意思。 如果是{'':20}can work\n",
        "    print(f\"{'':20s}{cur}:{rate:2f}\")\n",
        "    print(\"{:20s}{}:{:7.5f}\".format('',cur,rate))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asIMjTvzkAPx"
      },
      "source": [
        "# Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNDv2YAmhg-h"
      },
      "source": [
        "​\n",
        "array4 = []\n",
        "length_array1 = len(array1)\n",
        "for i in range(length_array1):\n",
        "    summed_number = array1[i] + array2[i]\n",
        "    array4.append(summed_number)\n",
        "array4 == array3\n",
        "​np.array(array1) + 6\n",
        "\n",
        "def timing_numpy():\n",
        "    return np.arange(big_n) + np.arange(big_n) \n",
        "\n",
        "\n",
        "\n",
        "## Insert your code\n",
        "np.array_equal(dow[np.where(dow[:,0]>12500),:].squeeze(), dow[dow[:,0]>12500,:])\n",
        "b = np.array(array2)\n",
        "# start end step\n",
        "a[0:-1:2,:]\n",
        "​"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXNq0oQNltsY"
      },
      "source": [
        "# details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BM7iM8Vlv4J"
      },
      "source": [
        "inplace=True \n",
        "#Inplace=true，直接在原来基础上修改， 如果是false，是重新按条件生成了一个df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}