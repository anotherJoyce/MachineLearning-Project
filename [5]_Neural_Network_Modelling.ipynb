{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[5] Neural Network Modelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th8hmRzQkMWu"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Sep 28 15:34:42 2020\n",
        "\n",
        "Purpose: AN6001 Project\n",
        "\n",
        "@author: vivek\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "os.chdir(\"C:/Users/vivek/Documents/MSBA/AN6001 Big Data and AI in Analytics/Group Project/nlp-getting-started\")\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "#%%\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "TrainData = pd.read_csv(\"train.csv\")\n",
        "TestData = pd.read_csv(\"test.csv\")\n",
        "SampleSubmitssionData = pd.read_csv(\"sample_submission.csv\")\n",
        "\n",
        "print(TrainData.head())\n",
        "print(TestData.head())\n",
        "print(SampleSubmitssionData.head())\n",
        "#%%\n",
        "import numpy as np\n",
        "print(TrainData)\n",
        "print(TestData)\n",
        "print(SampleSubmitssionData)\n",
        "\n",
        "#%%\n",
        "\n",
        "import nltk.classify.util\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop=set(stopwords.words('english'))\n",
        "\n",
        "print(stop)\n",
        "\n",
        "#%%\n",
        "\n",
        "def create_corpus(target):\n",
        "    corpus=[]\n",
        "    \n",
        "    for x in TrainData[TrainData['target']==target]['text'].str.split():\n",
        "        for i in x:\n",
        "            corpus.append(i)\n",
        "    return corpus\n",
        "\n",
        "corpus=create_corpus(0)\n",
        "\n",
        "#%%\n",
        "import string\n",
        "special = string.punctuation\n",
        "\n",
        "#%%\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "dic=defaultdict(int)\n",
        "for i in (corpus):\n",
        "    if i in special:\n",
        "        dic[i]+=1\n",
        "\n",
        "x,y=zip(*dic.items())\n",
        "plt.bar(x,y)\n",
        "\n",
        "#%%\n",
        "\n",
        "## Data Cleaning\n",
        "\n",
        "df=pd.concat([TrainData,TestData])\n",
        "df.shape\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "##Removing Urls\n",
        "\n",
        "import re\n",
        "example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"\n",
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub('',text)\n",
        "\n",
        "print(remove_URL(example))\n",
        "\n",
        "df['text']=df['text'].apply(lambda x : remove_URL(x))\n",
        "\n",
        "#%%\n",
        "\n",
        "## Removing HTML Tags\n",
        "\n",
        "example = \"\"\"<div>\n",
        "<h1>Real or Fake</h1>\n",
        "<p>Kaggle </p>\n",
        "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
        "</div>\"\"\"\n",
        "\n",
        "def remove_html(text):\n",
        "    html=re.compile(r'<.*?>')\n",
        "    return html.sub(r'',text)\n",
        "\n",
        "print(remove_html(example))\n",
        "\n",
        "df['text']=df['text'].apply(lambda x: remove_html(x))\n",
        "\n",
        "#%%\n",
        "\n",
        "## Removing Punctuations\n",
        "\n",
        "def remove_punct(text):\n",
        "    table=str.maketrans('','',string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "example=\"I am a #king\"\n",
        "print(remove_punct(example))\n",
        "\n",
        "df['text']=df['text'].apply(lambda x : remove_punct(x))\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "## Spelling Correction\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "from tqdm import tqdm\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Spell Correction, Lemmatization and Create corpus of Words\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "import json\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "def create_corpus(df):\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    corpus=[]\n",
        "    for tweet in tqdm(df['text']):\n",
        "        #print(tweet)\n",
        "        misspelled_words = spell.unknown(tweet.split())\n",
        "        corrected_text = [spell.correction(word) if word in misspelled_words else word for word in word_tokenize(tweet.lower())]\n",
        "        #print(corrected_text)\n",
        "        #tagged = nltk.pos_tag(corrected_text)\n",
        "        #wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), tagged)\n",
        "        #lemt_word = [word if tag is None else wordnet_lemmatizer.lemmatize(word, tag) for word, tag in wordnet_tagged]\n",
        "        #print(lemt_word)\n",
        "        words = [word for word in corrected_text if((word.isalpha()==1) & (word not in stop))]\n",
        "        corpus.append(words)\n",
        "        #print(words)\n",
        "    return corpus\n",
        "\n",
        "corpus=create_corpus(df)\n",
        "\n",
        "with open('corpus1.txt', 'w') as filehandle:\n",
        "    json.dump(corpus, filehandle)\n",
        "    \n",
        "#%%\n",
        "\n",
        "import json\n",
        "test = [[\"I\", \"am\", \"saving\", \"text\", \"array\"],[\"is\", \"it\", \"saved\"]]\n",
        "\n",
        "\n",
        "with open('output.txt', 'w') as filehandle:\n",
        "    json.dump(test, filehandle)\n",
        "\n",
        "check = []\n",
        "with open('corpus1.txt', 'r') as filehandle:\n",
        "    check = json.load(filehandle)\n",
        "\n",
        "corpus = check\n",
        "#print(check)\n",
        "\n",
        "#%%\n",
        "\n",
        "# Creating an embedded dictionary from Glove Txt file of 100 dimensions\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "embedding_dict={}\n",
        "with open('glove.6B/glove.6B.100d.txt','r', encoding=\"utf8\") as f:\n",
        "    for line in tqdm(f):\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()\n",
        "\n",
        "#%%\n",
        "\n",
        "# Creating Sequences of words from corpus\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "MAX_LEN=50\n",
        "tokenizer_obj=Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(corpus)\n",
        "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
        "\n",
        "#%%\n",
        "\n",
        "# Padding and storing Word Indexes\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
        "word_index=tokenizer_obj.word_index\n",
        "print('Number of unique words:',len(word_index))\n",
        "\n",
        "#%%\n",
        "\n",
        "# Creating Embdedding Matrix from Word Index and Embedding Dictionary \n",
        "# created from Globe Text\n",
        "\n",
        "num_words=len(word_index)+1\n",
        "embedding_matrix=np.zeros((num_words,100))\n",
        "\n",
        "for word,i in tqdm(word_index.items()):\n",
        "    emb_vec=embedding_dict.get(word)\n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i]=emb_vec\n",
        "        \n",
        "#%%\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import Constant\n",
        "\n",
        "model=Sequential()\n",
        "\n",
        "embedding=Embedding(num_words,100,\n",
        "                    embeddings_initializer=Constant(embedding_matrix),\n",
        "                   input_length=MAX_LEN,trainable=False)\n",
        "\n",
        "model.add(embedding)\n",
        "\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimzer=Adam(learning_rate=1e-5)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n",
        "\n",
        "#%%\n",
        "\n",
        "# Get the Trains and Test set from combined rows\n",
        "\n",
        "train=tweet_pad[:TrainData.shape[0]]\n",
        "test=tweet_pad[TrainData.shape[0]:]\n",
        "\n",
        "#train=df['text'][:TrainData.shape[0]]\n",
        "#test=df['text'][TrainData.shape[0]:]\n",
        "\n",
        "#%%\n",
        "\n",
        "# Divide the Train Data in Train and Validation Set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(train,\n",
        "                                               TrainData['target'].values,\n",
        "                                               test_size=0.15)\n",
        "print('Shape of train',X_train.shape)\n",
        "print(\"Shape of Validation \",X_test.shape)\n",
        "\n",
        "#%%\n",
        "\n",
        "# Train the model\n",
        "\n",
        "history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)\n",
        "\n",
        "#%%\n",
        "\n",
        "y_pred = model.predict(X_test, batch_size=64, verbose=1)\n",
        "#%%\n",
        "\n",
        "y_pred_bool = []\n",
        "for i in y_pred:\n",
        "    if i>0.5:\n",
        "        y_pred_bool.append(1)\n",
        "    else:\n",
        "        y_pred_bool.append(0)\n",
        "        \n",
        "#%%\n",
        "y_pred = model.predict(X_test, batch_size=64, verbose=1)\n",
        "y_train_pred = model.predict(X_train, batch_size=64, verbose=1)\n",
        "#%%\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred_bool = []\n",
        "for i in y_pred:\n",
        "    if i>0.5:\n",
        "        y_pred_bool.append(1)\n",
        "    else:\n",
        "        y_pred_bool.append(0)\n",
        "\n",
        "y_train_pred_bool = []\n",
        "for i in y_train_pred:\n",
        "    if i>0.5:\n",
        "        y_train_pred_bool.append(1)\n",
        "    else:\n",
        "        y_train_pred_bool.append(0)\n",
        "#%%\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}